version: '3.8'

services:
  vllm:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: 12.1.1
        PYTHON_VERSION: 3.12
        max_jobs: 1
        nvcc_threads: 2
        torch_cuda_arch_list: "8.0 9.0a"  # 仅支持 A100/A800/H20/H100
    image: vllm:cuda12.1-datacenter
    container_name: vllm-datacenter
    ports:
      - "8000:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=all
      - VLLM_USAGE_SOURCE=docker-compose
      - HF_HUB_ENABLE_HF_TRANSFER=1
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    # 针对数据中心显卡的大模型推理配置
    command: 
      - "--model"
      - "Qwen/Qwen2.5-7B-Instruct"
      - "--host" 
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--tensor-parallel-size"
      - "2"
      - "--max-model-len"
      - "16384"
      - "--enforce-eager"
    runtime: nvidia
    shm_size: 16g
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # 使用所有可用 GPU
              capabilities: [gpu]

  # 单 GPU 配置（适用于单卡 A100/A800）
  vllm-single:
    build:
      context: .
      dockerfile: Dockerfile
      args:
        CUDA_VERSION: 12.1.1
        PYTHON_VERSION: 3.12
        max_jobs: 1
        nvcc_threads: 2
        torch_cuda_arch_list: "8.0 9.0a"
    image: vllm:cuda12.1-datacenter
    container_name: vllm-single-gpu
    ports:
      - "8001:8000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - VLLM_USAGE_SOURCE=docker-compose
      - HF_HUB_ENABLE_HF_TRANSFER=1
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: 
      - "--model"
      - "Qwen/Qwen2.5-7B-Instruct"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
      - "--max-model-len"
      - "8192"
    runtime: nvidia
    shm_size: 8g
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - single-gpu
